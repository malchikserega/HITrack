import requests
import logging
import time
from typing import Dict, List, Optional, Tuple, Set
from datetime import datetime, date
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
from pathlib import Path

logger = logging.getLogger(__name__)

class VulnerabilityDataCollector:
    """
    Collects vulnerability information from various external sources.
    """
    
    def __init__(self, cache_dir: str = None, max_workers: int = 5):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'HITrack-Vulnerability-Scanner/1.0'
        })
        
        # Cache settings
        self.cache_dir = cache_dir or "/tmp/hitrack_cache"
        Path(self.cache_dir).mkdir(exist_ok=True)
        
        # Threading settings
        self.max_workers = max_workers
        
        # CISA KEV cache
        self._cisa_kev_cache = None
        self._cisa_kev_cache_time = None
        self._cisa_kev_lock = threading.Lock()
        
        # Exploit-DB cache
        self._exploit_db_cache = None
        self._exploit_db_cache_time = None
        self._exploit_db_lock = threading.Lock()
        
        # EPSS cache
        self._epss_cache = {}
        self._epss_cache_time = None
        self._epss_lock = threading.Lock()
        
        # Rate limiting
        self._last_request_time = {}
        self._rate_limit_lock = threading.Lock()
    
    def get_cve_details(self, cve_id: str) -> Optional[Dict]:
        """
        Get detailed information about a CVE from CVE Details.
        
        Args:
            cve_id: The CVE identifier (e.g., 'CVE-2021-44228')
            
        Returns:
            Dictionary with CVE details or None if not found
        """
        try:
            self._rate_limit("cve.circl.lu", 1.0)
            # CVE Details API endpoint
            url = f"https://cve.circl.lu/api/cve/{cve_id}"
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            if not data:
                logger.warning(f"No data returned for {cve_id}")
                return None
            
            # Extract relevant information
            cve_details = {
                'cve_details_score': self._extract_cvss_score(data),
                'cve_details_severity': self._extract_severity(data),
                'cve_details_published_date': self._extract_published_date(data),
                'cve_details_updated_date': self._extract_updated_date(data),
                'cve_details_summary': self._extract_summary(data),
                'cve_details_references': self._extract_references(data),
            }
            
            logger.info(f"Extracted CVE details for {cve_id}: {cve_details}")
            return cve_details
            
        except requests.RequestException as e:
            logger.warning(f"Failed to get CVE details for {cve_id}: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error processing CVE details for {cve_id}: {str(e)}")
            return None
    
    def get_exploit_info(self, cve_id: str) -> Optional[Dict]:
        """
        Get exploit information for a CVE from various sources.
        
        Args:
            cve_id: The CVE identifier
            
        Returns:
            Dictionary with exploit information or None if not found
        """
        logger.info(f"Starting exploit info collection for {cve_id}")
        
        try:
            exploit_info = {
                'exploit_available': False,
                'exploit_public': False,
                'exploit_verified': False,
                'exploit_links': [],
                # Separate Exploit-DB information
                'exploit_db_available': False,
                'exploit_db_verified': False,
                'exploit_db_count': 0,
                'exploit_db_verified_count': 0,
                'exploit_db_working_count': 0,
                'exploit_db_links': []
            }
            
            # Check CISA KEV first (most authoritative)
            logger.debug(f"Checking CISA KEV for {cve_id}")
            cisa_kev_info = self._check_cisa_kev(cve_id)
            if cisa_kev_info:
                exploit_info.update(cisa_kev_info)
                # If found in CISA KEV, mark as exploited and verified
                exploit_info['exploit_available'] = True
                exploit_info['exploit_verified'] = True
                exploit_info['exploit_public'] = True
                logger.info(f"CVE {cve_id} found in CISA KEV - marking as verified exploit")
            else:
                logger.debug(f"CVE {cve_id} not found in CISA KEV")
            
            # Check Exploit-DB (always check for separate tracking)
            logger.debug(f"Checking Exploit-DB for {cve_id}")
            exploit_db_info = self._check_exploit_db(cve_id)
            if exploit_db_info:
                # Store Exploit-DB specific information separately
                exploit_info['exploit_db_available'] = exploit_db_info.get('exploit_available', False)
                exploit_info['exploit_db_verified'] = exploit_db_info.get('exploit_verified', False)
                exploit_info['exploit_db_count'] = exploit_db_info.get('exploit_count', 0)
                exploit_info['exploit_db_verified_count'] = exploit_db_info.get('verified_count', 0)
                exploit_info['exploit_db_working_count'] = exploit_db_info.get('working_count', 0)
                exploit_info['exploit_db_links'] = exploit_db_info.get('links', [])
                
                # Only mark as general exploit if not already marked by CISA KEV
                if not cisa_kev_info:
                    exploit_info['exploit_available'] = exploit_db_info.get('exploit_available', False)
                    exploit_info['exploit_verified'] = exploit_db_info.get('exploit_verified', False)
                    exploit_info['exploit_links'].extend(exploit_db_info.get('links', []))
                
                logger.info(f"CVE {cve_id} found in Exploit-DB - separate tracking enabled")
                logger.debug(f"Exploit-DB details: {exploit_db_info}")
            else:
                logger.debug(f"CVE {cve_id} not found in Exploit-DB")
            
            # Check NVD (for vulnerability information only)
            logger.debug(f"Checking NVD for {cve_id}")
            nvd_info = self._check_nvd(cve_id)
            if nvd_info:
                exploit_info['exploit_links'].extend(nvd_info.get('links', []))
                # Don't mark as exploit based on NVD alone
                logger.debug(f"Found {len(nvd_info.get('links', []))} NVD links for {cve_id}")
            else:
                logger.debug(f"No NVD information found for {cve_id}")
            
            logger.info(f"Extracted exploit info for {cve_id}: {exploit_info}")
            return exploit_info
            
        except Exception as e:
            logger.error(f"Error getting exploit info for {cve_id}: {str(e)}")
            return None
    
    def _extract_cvss_score(self, data: Dict) -> Optional[float]:
        """Extract CVSS score from CVE data."""
        try:
            # Try different possible field names (old format)
            cvss_fields = ['cvss', 'cvss_score', 'score', 'cvss_v3_score']
            for field in cvss_fields:
                if field in data and data[field]:
                    try:
                        return float(data[field])
                    except (ValueError, TypeError):
                        continue
            
            # Try CVE API v5 format - check both cna and adp containers
            if 'containers' in data and data['containers']:
                containers = data['containers']
                
                # Check cna container first (where CVSS data is usually found)
                if 'cna' in containers and containers['cna']:
                    cna = containers['cna']
                    # Handle both list and dict formats
                    if isinstance(cna, list):
                        cna_items = cna
                    else:
                        cna_items = [cna]
                    
                    for cna_item in cna_items:
                        if isinstance(cna_item, dict) and 'metrics' in cna_item and cna_item['metrics']:
                            metrics = cna_item['metrics']
                            if isinstance(metrics, list) and metrics:
                                for metric in metrics:
                                    if isinstance(metric, dict):
                                        # Try CVSS v3.1 first
                                        if 'cvssV3_1' in metric and metric['cvssV3_1']:
                                            cvss_data = metric['cvssV3_1']
                                            if isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                                try:
                                                    return float(cvss_data['baseScore'])
                                                except (ValueError, TypeError):
                                                    pass
                                        # Try CVSS v3.0
                                        elif 'cvssV3_0' in metric and metric['cvssV3_0']:
                                            cvss_data = metric['cvssV3_0']
                                            if isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                                try:
                                                    return float(cvss_data['baseScore'])
                                                except (ValueError, TypeError):
                                                    pass
                                        # Try CVSS v2
                                        elif 'cvssV2' in metric and metric['cvssV2']:
                                            cvss_data = metric['cvssV2']
                                            if isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                                try:
                                                    return float(cvss_data['baseScore'])
                                                except (ValueError, TypeError):
                                                    pass
                            elif isinstance(metrics, dict):
                                # Try CVSS v3
                                for cvss_version in ['cvssV3_1', 'cvssV3_0']:
                                    if cvss_version in metrics and metrics[cvss_version]:
                                        cvss_data = metrics[cvss_version]
                                        if isinstance(cvss_data, list) and cvss_data:
                                            cvss_item = cvss_data[0]
                                            if 'baseScore' in cvss_item:
                                                try:
                                                    return float(cvss_item['baseScore'])
                                                except (ValueError, TypeError):
                                                    pass
                                        elif isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                            try:
                                                return float(cvss_data['baseScore'])
                                            except (ValueError, TypeError):
                                                pass
                                # Try CVSS v2
                                if 'cvssV2' in metrics and metrics['cvssV2']:
                                    cvss_data = metrics['cvssV2']
                                    if isinstance(cvss_data, list) and cvss_data:
                                        cvss_item = cvss_data[0]
                                        if 'baseScore' in cvss_item:
                                            try:
                                                return float(cvss_item['baseScore'])
                                            except (ValueError, TypeError):
                                                pass
                                    elif isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                        try:
                                            return float(cvss_data['baseScore'])
                                        except (ValueError, TypeError):
                                            pass
                
                # Check adp container as fallback
                if 'adp' in containers and containers['adp']:
                    adp = containers['adp']
                    if isinstance(adp, list) and adp:
                        for adp_item in adp:
                            if isinstance(adp_item, dict) and 'metrics' in adp_item and adp_item['metrics']:
                                metrics = adp_item['metrics']
                                if isinstance(metrics, list) and metrics:
                                    for metric in metrics:
                                        if isinstance(metric, dict):
                                            # Try CVSS v3.1 first
                                            if 'cvssV3_1' in metric and metric['cvssV3_1']:
                                                cvss_data = metric['cvssV3_1']
                                                if isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                                    try:
                                                        return float(cvss_data['baseScore'])
                                                    except (ValueError, TypeError):
                                                        pass
                                            # Try CVSS v3.0
                                            elif 'cvssV3_0' in metric and metric['cvssV3_0']:
                                                cvss_data = metric['cvssV3_0']
                                                if isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                                    try:
                                                        return float(cvss_data['baseScore'])
                                                    except (ValueError, TypeError):
                                                        pass
                                            # Try CVSS v2
                                            elif 'cvssV2' in metric and metric['cvssV2']:
                                                cvss_data = metric['cvssV2']
                                                if isinstance(cvss_data, dict) and 'baseScore' in cvss_data:
                                                    try:
                                                        return float(cvss_data['baseScore'])
                                                    except (ValueError, TypeError):
                                                        pass
            
            # Try old format (fallback)
            if 'cvss_v3' in data and data['cvss_v3']:
                if isinstance(data['cvss_v3'], dict) and 'base_score' in data['cvss_v3']:
                    try:
                        return float(data['cvss_v3']['base_score'])
                    except (ValueError, TypeError):
                        pass
            
            return None
        except (ValueError, TypeError):
            return None
    
    def _extract_severity(self, data: Dict) -> Optional[str]:
        """Extract severity from CVE data."""
        try:
            # Try to get severity from CVSS score
            score = self._extract_cvss_score(data)
            if score:
                if score >= 9.0:
                    return 'CRITICAL'
                elif score >= 7.0:
                    return 'HIGH'
                elif score >= 4.0:
                    return 'MEDIUM'
                else:
                    return 'LOW'
            
            # Try direct severity field
            severity_fields = ['severity', 'cvss_severity', 'impact']
            for field in severity_fields:
                if field in data and data[field]:
                    severity = str(data[field]).upper()
                    if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                        return severity
            
            # Try CVE API v5 format - check both cna and adp containers
            if 'containers' in data and data['containers']:
                containers = data['containers']
                
                # Check cna container first (where CVSS data is usually found)
                if 'cna' in containers and containers['cna']:
                    cna = containers['cna']
                    # Handle both list and dict formats
                    if isinstance(cna, list):
                        cna_items = cna
                    else:
                        cna_items = [cna]
                    
                    for cna_item in cna_items:
                        if isinstance(cna_item, dict) and 'metrics' in cna_item and cna_item['metrics']:
                            metrics = cna_item['metrics']
                            if isinstance(metrics, list) and metrics:
                                for metric in metrics:
                                    if isinstance(metric, dict):
                                        # Try CVSS v3.1 severity
                                        if 'cvssV3_1' in metric and metric['cvssV3_1']:
                                            cvss_data = metric['cvssV3_1']
                                            if isinstance(cvss_data, dict) and 'baseSeverity' in cvss_data:
                                                severity = str(cvss_data['baseSeverity']).upper()
                                                if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                                    return severity
                                        # Try CVSS v3.0 severity
                                        elif 'cvssV3_0' in metric and metric['cvssV3_0']:
                                            cvss_data = metric['cvssV3_0']
                                            if isinstance(cvss_data, dict) and 'baseSeverity' in cvss_data:
                                                severity = str(cvss_data['baseSeverity']).upper()
                                                if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                                    return severity
                            elif isinstance(metrics, dict):
                                # Try CVSS v3 severity
                                for cvss_version in ['cvssV3_1', 'cvssV3_0']:
                                    if cvss_version in metrics and metrics[cvss_version]:
                                        cvss_data = metrics[cvss_version]
                                        if isinstance(cvss_data, list) and cvss_data:
                                            cvss_item = cvss_data[0]
                                            if 'baseSeverity' in cvss_item:
                                                severity = str(cvss_item['baseSeverity']).upper()
                                                if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                                    return severity
                                        elif isinstance(cvss_data, dict) and 'baseSeverity' in cvss_data:
                                            severity = str(cvss_data['baseSeverity']).upper()
                                            if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                                return severity
                
                # Check adp container as fallback
                if 'adp' in containers and containers['adp']:
                    adp = containers['adp']
                    if isinstance(adp, list) and adp:
                        for adp_item in adp:
                            if isinstance(adp_item, dict) and 'metrics' in adp_item and adp_item['metrics']:
                                metrics = adp_item['metrics']
                                if isinstance(metrics, list) and metrics:
                                    for metric in metrics:
                                        if isinstance(metric, dict):
                                            # Try CVSS v3.1 severity
                                            if 'cvssV3_1' in metric and metric['cvssV3_1']:
                                                cvss_data = metric['cvssV3_1']
                                                if isinstance(cvss_data, dict) and 'baseSeverity' in cvss_data:
                                                    severity = str(cvss_data['baseSeverity']).upper()
                                                    if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                                        return severity
                                            # Try CVSS v3.0 severity
                                            elif 'cvssV3_0' in metric and metric['cvssV3_0']:
                                                cvss_data = metric['cvssV3_0']
                                                if isinstance(cvss_data, dict) and 'baseSeverity' in cvss_data:
                                                    severity = str(cvss_data['baseSeverity']).upper()
                                                    if severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
                                                        return severity
            
            return None
        except (ValueError, TypeError):
            return None
    
    def _extract_published_date(self, data: Dict) -> Optional[date]:
        """Extract published date from CVE data."""
        try:
            date_fields = ['Published', 'published', 'created', 'created_at', 'published_date']
            for field in date_fields:
                if field in data and data[field]:
                    try:
                        date_str = str(data[field])
                        # Try different date formats
                        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ']:
                            try:
                                return datetime.strptime(date_str, fmt).date()
                            except ValueError:
                                continue
                    except (ValueError, TypeError):
                        continue
            
            # Try CVE API v5 format
            if 'cveMetadata' in data and data['cveMetadata']:
                metadata = data['cveMetadata']
                if 'datePublished' in metadata and metadata['datePublished']:
                    try:
                        date_str = str(metadata['datePublished'])
                        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ']:
                            try:
                                return datetime.strptime(date_str, fmt).date()
                            except ValueError:
                                continue
                    except (ValueError, TypeError):
                        pass
            
            return None
        except (ValueError, TypeError):
            return None
    
    def _extract_updated_date(self, data: Dict) -> Optional[date]:
        """Extract updated date from CVE data."""
        try:
            date_fields = ['Modified', 'modified', 'updated', 'updated_at', 'modified_date']
            for field in date_fields:
                if field in data and data[field]:
                    try:
                        date_str = str(data[field])
                        # Try different date formats
                        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ']:
                            try:
                                return datetime.strptime(date_str, fmt).date()
                            except ValueError:
                                continue
                    except (ValueError, TypeError):
                        continue
            
            # Try CVE API v5 format
            if 'cveMetadata' in data and data['cveMetadata']:
                metadata = data['cveMetadata']
                if 'dateUpdated' in metadata and metadata['dateUpdated']:
                    try:
                        date_str = str(metadata['dateUpdated'])
                        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%SZ']:
                            try:
                                return datetime.strptime(date_str, fmt).date()
                            except ValueError:
                                continue
                    except (ValueError, TypeError):
                        pass
            
            return None
        except (ValueError, TypeError):
            return None
    
    def _extract_summary(self, data: Dict) -> Optional[str]:
        """Extract summary from CVE data."""
        try:
            summary_fields = ['summary', 'description', 'details', 'overview']
            for field in summary_fields:
                if field in data and data[field]:
                    summary = str(data[field]).strip()
                    if summary:
                        return summary
            
            # Try CVE API v5 format
            if 'containers' in data and data['containers']:
                containers = data['containers']
                if 'cna' in containers and containers['cna']:
                    cna = containers['cna']
                    # Handle both list and dict formats
                    if isinstance(cna, list):
                        cna_items = cna
                    else:
                        cna_items = [cna]
                    
                    for cna_item in cna_items:
                        if isinstance(cna_item, dict) and 'descriptions' in cna_item and cna_item['descriptions']:
                            descriptions = cna_item['descriptions']
                            if isinstance(descriptions, list) and descriptions:
                                for desc in descriptions:
                                    if isinstance(desc, dict) and 'value' in desc:
                                        summary = str(desc['value']).strip()
                                        if summary:
                                            return summary
            
            return None
        except (ValueError, TypeError):
            return None
    
    def _extract_references(self, data: Dict) -> List[str]:
        """Extract references from CVE data."""
        try:
            references = []
            ref_fields = ['references', 'refs', 'links', 'urls']
            
            for field in ref_fields:
                if field in data and data[field]:
                    refs = data[field]
                    if isinstance(refs, list):
                        for ref in refs:
                            if isinstance(ref, dict):
                                # Try different possible URL fields
                                url_fields = ['url', 'link', 'href', 'reference']
                                for url_field in url_fields:
                                    if url_field in ref and ref[url_field]:
                                        url = str(ref[url_field]).strip()
                                        if url and url not in references:
                                            references.append(url)
                                        break
                            elif isinstance(ref, str):
                                url = ref.strip()
                                if url and url not in references:
                                    references.append(url)
            
            # Try CVE API v5 format
            if 'containers' in data and data['containers']:
                containers = data['containers']
                if 'cna' in containers and containers['cna']:
                    cna = containers['cna']
                    # Handle both list and dict formats
                    if isinstance(cna, list):
                        cna_items = cna
                    else:
                        cna_items = [cna]
                    
                    for cna_item in cna_items:
                        if isinstance(cna_item, dict) and 'references' in cna_item and cna_item['references']:
                            refs = cna_item['references']
                            if isinstance(refs, list):
                                for ref in refs:
                                    if isinstance(ref, dict):
                                        if 'url' in ref and ref['url']:
                                            url = str(ref['url']).strip()
                                            if url and url not in references:
                                                references.append(url)
            
            return references
        except (ValueError, TypeError):
            return []
    
    def _check_exploit_db(self, cve_id: str) -> Optional[Dict]:
        """Check Exploit-DB for exploit information."""
        try:
            # Use bulk method for single CVE
            bulk_results = self._check_exploit_db_bulk({cve_id})
            return bulk_results.get(cve_id)
            
        except Exception as e:
            logger.debug(f"Failed to check Exploit-DB for {cve_id}: {str(e)}")
            return None
    
    def _check_github_advisories(self, cve_id: str) -> Optional[Dict]:
        """Check GitHub Security Advisories for exploit information."""
        try:
            # GitHub Security Advisories API requires authentication
            # For now, we'll skip this check as it requires a GitHub token
            # TODO: Implement GitHub token authentication if needed
            logger.debug(f"Skipping GitHub advisories check for {cve_id} - requires authentication")
            return None
            
        except Exception as e:
            logger.debug(f"Failed to check GitHub advisories for {cve_id}: {str(e)}")
            return None
    
    def _check_nvd(self, cve_id: str) -> Optional[Dict]:
        """Check NVD for exploit information."""
        try:
            # Use bulk method for single CVE
            bulk_results = self._check_nvd_bulk({cve_id})
            return bulk_results.get(cve_id)
            
        except Exception as e:
            logger.debug(f"Failed to check NVD for {cve_id}: {str(e)}")
            return None
    
    def _rate_limit(self, domain: str, delay: float = 1.0):
        """Rate limiting per domain."""
        with self._rate_limit_lock:
            now = time.time()
            if domain in self._last_request_time:
                time_since_last = now - self._last_request_time[domain]
                if time_since_last < delay:
                    time.sleep(delay - time_since_last)
            self._last_request_time[domain] = time.time()
    
    def _get_cached_cisa_kev(self) -> Optional[Dict]:
        """Get CISA KEV data with caching."""
        cache_file = Path(self.cache_dir) / "cisa_kev_cache.json"
        cache_time_file = Path(self.cache_dir) / "cisa_kev_cache_time.txt"
        
        # Check if cache exists and is fresh (less than 1 hour old)
        if cache_file.exists() and cache_time_file.exists():
            try:
                cache_time = float(cache_time_file.read_text().strip())
                if time.time() - cache_time < 3600:  # 1 hour cache
                    with open(cache_file, 'r') as f:
                        return json.load(f)
            except:
                pass
        
        return None
    
    def _save_cisa_kev_cache(self, data: Dict):
        """Save CISA KEV data to cache."""
        cache_file = Path(self.cache_dir) / "cisa_kev_cache.json"
        cache_time_file = Path(self.cache_dir) / "cisa_kev_cache_time.txt"
        
        try:
            with open(cache_file, 'w') as f:
                json.dump(data, f)
            with open(cache_time_file, 'w') as f:
                f.write(str(time.time()))
        except Exception as e:
            logger.warning(f"Failed to save CISA KEV cache: {e}")
    
    def clear_exploit_db_cache(self):
        """Clear the Exploit-DB cache to force a fresh download."""
        with self._exploit_db_lock:
            self._exploit_db_cache = None
            self._exploit_db_cache_time = None
            logger.info("Exploit-DB cache cleared")
    
    def get_exploit_db_cache_info(self) -> Dict:
        """Get information about the current Exploit-DB cache status."""
        with self._exploit_db_lock:
            if self._exploit_db_cache is not None and self._exploit_db_cache_time is not None:
                cache_age = time.time() - self._exploit_db_cache_time
                cache_size_mb = len(self._exploit_db_cache.encode('utf-8')) / (1024 * 1024)
                return {
                    'cached': True,
                    'age_minutes': cache_age / 60,
                    'age_hours': cache_age / 3600,
                    'size_mb': cache_size_mb,
                    'expires_in_minutes': max(0, (3600 - cache_age) / 60)
                }
            else:
                return {
                    'cached': False,
                    'age_minutes': None,
                    'age_hours': None,
                    'size_mb': None,
                    'expires_in_minutes': None
                }
    
    def _get_cached_epss(self, cve_ids: List[str]) -> Dict[str, Dict]:
        """Get cached EPSS data for specific CVEs."""
        with self._epss_lock:
            if (self._epss_cache_time is not None and 
                time.time() - self._epss_cache_time < 86400):  # 24 hour cache
                
                cached_results = {}
                for cve_id in cve_ids:
                    if cve_id in self._epss_cache:
                        cached_results[cve_id] = self._epss_cache[cve_id]
                
                if cached_results:
                    logger.debug(f"Using cached EPSS data for {len(cached_results)} CVEs")
                    return cached_results
            
            return {}
    
    def _save_epss_cache(self, epss_data: Dict[str, Dict]):
        """Save EPSS data to cache."""
        with self._epss_lock:
            self._epss_cache.update(epss_data)
            self._epss_cache_time = time.time()
            logger.debug(f"Updated EPSS cache with {len(epss_data)} entries")
    
    def clear_epss_cache(self):
        """Clear the EPSS cache to force a fresh download."""
        with self._epss_lock:
            self._epss_cache.clear()
            self._epss_cache_time = None
            logger.info("EPSS cache cleared")
    
    def get_epss_cache_info(self) -> Dict:
        """Get information about the current EPSS cache status."""
        with self._epss_lock:
            if self._epss_cache_time is not None:
                cache_age = time.time() - self._epss_cache_time
                return {
                    'cached': True,
                    'age_minutes': cache_age / 60,
                    'age_hours': cache_age / 3600,
                    'cached_cves': len(self._epss_cache),
                    'expires_in_minutes': max(0, (86400 - cache_age) / 60)
                }
            else:
                return {
                    'cached': False,
                    'age_minutes': None,
                    'age_hours': None,
                    'cached_cves': 0,
                    'expires_in_minutes': None
                }
    
    def _load_cisa_kev_data(self) -> Optional[Dict]:
        """Load CISA KEV data with caching."""
        # Try cache first
        cached_data = self._get_cached_cisa_kev()
        if cached_data:
            logger.info("Using cached CISA KEV data")
            return cached_data
        
        # Fetch fresh data
        try:
            self._rate_limit("cisa.gov", 2.0)  # Be extra respectful to CISA
            url = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            # Save to cache
            self._save_cisa_kev_cache(data)
            
            logger.info(f"Fetched fresh CISA KEV data: {data.get('count', 0)} vulnerabilities")
            return data
            
        except Exception as e:
            logger.error(f"Failed to load CISA KEV data: {e}")
            return None
    
    def _check_cisa_kev_bulk(self, cve_ids: Set[str]) -> Dict[str, Dict]:
        """Check multiple CVEs against CISA KEV in one operation."""
        results = {}
        
        # Load CISA KEV data once
        kev_data = self._load_cisa_kev_data()
        if not kev_data or 'vulnerabilities' not in kev_data:
            return results
        
        # Create lookup dictionary for O(1) access
        kev_lookup = {}
        for vuln in kev_data['vulnerabilities']:
            cve_id = vuln.get('cveID')
            if cve_id:
                kev_lookup[cve_id] = vuln
        
        # Check each CVE
        for cve_id in cve_ids:
            if cve_id in kev_lookup:
                vuln = kev_lookup[cve_id]
                results[cve_id] = {
                    'cisa_kev_known_exploited': True,
                    'cisa_kev_date_added': vuln.get('dateAdded'),
                    'cisa_kev_vendor_project': vuln.get('vendorProject'),
                    'cisa_kev_product': vuln.get('product'),
                    'cisa_kev_vulnerability_name': vuln.get('vulnerabilityName'),
                    'cisa_kev_short_description': vuln.get('shortDescription'),
                    'cisa_kev_required_action': vuln.get('requiredAction'),
                    'cisa_kev_due_date': vuln.get('dueDate'),
                    'cisa_kev_ransomware_use': vuln.get('knownRansomwareCampaignUse'),
                    'cisa_kev_notes': vuln.get('notes'),
                    'cisa_kev_cwes': vuln.get('cwes', []),
                    'links': [f"https://www.cisa.gov/known-exploited-vulnerabilities-catalog?field_cve={cve_id}"]
                }
        
        return results
    
    def _check_cisa_kev(self, cve_id: str) -> Optional[Dict]:
        """Check CISA KEV catalog for known exploited vulnerabilities."""
        try:
            # Use bulk method for single CVE
            bulk_results = self._check_cisa_kev_bulk({cve_id})
            return bulk_results.get(cve_id)
            
        except Exception as e:
            logger.debug(f"Failed to check CISA KEV for {cve_id}: {str(e)}")
            return None
    
    def _check_exploit_db_bulk(self, cve_ids: Set[str]) -> Dict[str, Dict]:
        """Check multiple CVEs against Exploit-DB using CSV file with memory optimization."""
        results = {}
        
        logger.info(f"Starting Exploit-DB bulk check for {len(cve_ids)} CVEs: {list(cve_ids)[:5]}{'...' if len(cve_ids) > 5 else ''}")
        
        try:
            # Check cache first
            with self._exploit_db_lock:
                current_time = time.time()
                cache_age = None
                
                if (self._exploit_db_cache is not None and 
                    self._exploit_db_cache_time is not None):
                    cache_age = current_time - self._exploit_db_cache_time
                    
                    # Use cache if it's less than 1 hour old
                    if cache_age < 3600:  # 1 hour
                        csv_content = self._exploit_db_cache
                        logger.info(f"Using cached Exploit-DB CSV data (age: {cache_age/60:.1f} minutes)")
                        use_streaming = False
                    else:
                        logger.info(f"Cache expired (age: {cache_age/3600:.1f} hours), will refresh")
                        csv_content = None
                        use_streaming = False
                else:
                    logger.info("No cached data available, will download")
                    csv_content = None
                    use_streaming = False
                
                if csv_content is None:
                    # Download and parse the CSV file from Exploit-DB repository
                    csv_url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"
                    
                    logger.info(f"Downloading Exploit-DB CSV from: {csv_url}")
                    response = self.session.get(csv_url, timeout=30, stream=True)
                    response.raise_for_status()
                    
                    # Get file size for logging
                    content_length = response.headers.get('content-length')
                    if content_length:
                        file_size_mb = int(content_length) / (1024 * 1024)
                        logger.info(f"CSV file size: {file_size_mb:.2f} MB")
                    else:
                        file_size_mb = 0
                        logger.warning("No content-length header available")
                    
                    # Always cache the file content for reuse, regardless of size
                    logger.info("Downloading and caching Exploit-DB CSV data")
                    csv_content = response.text
                    self._exploit_db_cache = csv_content
                    self._exploit_db_cache_time = current_time
                    
                    # Close the response
                    response.close()
                    logger.info("CSV file downloaded and cached successfully")
            
            # Process cached content (most efficient for multiple CVEs)
            logger.info("Processing Exploit-DB CSV using cached content")
            results = self._process_exploit_db_content(csv_content, cve_ids)
            
            logger.info(f"Exploit-DB bulk check completed. Found exploits for {len(results)} CVEs: {list(results.keys())}")
            return results
            
        except Exception as e:
            logger.error(f"Failed to download or parse Exploit-DB CSV file: {str(e)}")
            return results
    
    def _process_exploit_db_content(self, csv_content: str, cve_ids: Set[str]) -> Dict[str, Dict]:
        """Process cached CSV content."""
        results = {}
        lines = csv_content.split('\n')
        
        logger.info(f"Processing {len(lines)} lines from Exploit-DB CSV for {len(cve_ids)} CVEs")
        
        # Find header and process
        header_info = self._parse_csv_header(lines)
        if not header_info:
            logger.error("Failed to parse CSV header from Exploit-DB CSV")
            return results
        
        logger.debug(f"CSV header parsed successfully: {header_info}")
        
        # Process each line
        processed_lines = 0
        found_exploits = 0
        cves_found = set()  # Track which CVEs we've found
        
        for line in lines[1:]:  # Skip header
            if not line.strip():
                continue
            
            processed_lines += 1
            
            # Process the line
            initial_results_count = len(results)
            self._process_csv_line(line, header_info, cve_ids, results)
            
            # Check if we found new exploits
            if len(results) > initial_results_count:
                found_exploits += 1
                # Update our tracking set
                cves_found.update(results.keys())
            
            # Early exit if we found all CVEs
            if len(results) == len(cve_ids):
                logger.info(f"Found all {len(cve_ids)} CVEs, stopping early at line {processed_lines}")
                break
            
            # Log progress every 1000 lines
            if processed_lines % 1000 == 0:
                logger.debug(f"Processed {processed_lines} lines, found exploits for {len(results)} CVEs")
                
                # Additional early exit check: if we've found most CVEs, continue a bit more
                if len(results) >= len(cve_ids) * 0.8:  # Found 80% of CVEs
                    logger.info(f"Found {len(results)} out of {len(cve_ids)} CVEs ({len(results)/len(cve_ids)*100:.1f}%), continuing...")
        
        logger.info(f"Processed {processed_lines} lines from Exploit-DB CSV. Found exploits for {len(results)} CVEs out of {len(cve_ids)} requested")
        
        # Log which CVEs were found and which were missed
        if results:
            found_cves = list(results.keys())
            missed_cves = list(cve_ids - set(found_cves))
            logger.info(f"Found CVEs: {found_cves}")
            if missed_cves:
                logger.info(f"Missed CVEs: {missed_cves}")
        
        return results
    
    def _process_exploit_db_streaming(self, response, cve_ids: Set[str]) -> Dict[str, Dict]:
        """Process CSV file using streaming to save memory."""
        results = {}
        header_found = False
        header_info = None
        lines_processed = 0
        lines_checked = 0
        
        logger.info(f"Starting streaming processing for {len(cve_ids)} CVEs")
        
        try:
            # Process file line by line
            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue
                
                lines_checked += 1
                
                if not header_found:
                    # Look for header line
                    logger.debug(f"Line {lines_checked}: Looking for header in: {line[:100]}...")
                    
                    # Check for exact header match first
                    if 'id,file,description,date_published,author,type,platform,port,date_added,date_updated,verified,codes' in line:
                        header_info = self._parse_csv_header([line])
                        header_found = True
                        if not header_info:
                            logger.error("Failed to parse CSV header")
                            break
                        logger.info(f"Found CSV header at line {lines_checked}: {header_info}")
                        continue
                    else:
                        # Alternative: check if line contains all required column names
                        required_columns = ['id', 'verified', 'codes', 'type']
                        if all(col in line for col in required_columns):
                            logger.info(f"Found alternative header format at line {lines_checked}: {line[:100]}...")
                            header_info = self._parse_csv_header([line])
                            header_found = True
                            if not header_info:
                                logger.error("Failed to parse alternative CSV header")
                                break
                            logger.info(f"Successfully parsed alternative header: {header_info}")
                            continue
                        else:
                            logger.debug(f"Line {lines_checked}: Skipping non-header line: {line[:50]}...")
                            # Stop looking for header after 100 lines to avoid infinite loop
                            if lines_checked > 100:
                                logger.error("Failed to find header after 100 lines, stopping")
                                break
                            continue  # Skip lines until we find header
            
                # Process data line
                logger.debug(f"Processing data line {lines_processed + 1}: {line[:100]}...")
                self._process_csv_line(line, header_info, cve_ids, results)
                lines_processed += 1
                
                # Early exit if we found all CVEs
                if len(results) == len(cve_ids):
                    logger.info(f"Found all {len(cve_ids)} CVEs after processing {lines_processed} lines, stopping early")
                    break
                
                # Log progress for large files
                if lines_processed % 10000 == 0:
                    logger.info(f"Processed {lines_processed} lines, found {len(results)} CVEs")
                
                # Safety check to prevent infinite loops
                if lines_processed > 1000000:  # 1 million lines max
                    logger.warning(f"Reached maximum line limit ({lines_processed}), stopping processing")
                    break
                    
        except Exception as e:
            logger.error(f"Error during streaming processing: {str(e)}")
            logger.error(f"Processed {lines_processed} lines before error, checked {lines_checked} lines")
        
        logger.info(f"Processed Exploit-DB CSV stream ({lines_processed} lines, checked {lines_checked} lines), found exploits for {len(results)} CVEs")
        return results
    
    def _parse_csv_header(self, lines: List[str]) -> Optional[Dict]:
        """Parse CSV header and return column indices."""
        for line in lines:
            # Check for the actual header format from Exploit-DB
            if 'id,file,description,date_published,author,type,platform,port,date_added,date_updated,verified,codes' in line:
                headers = line.split(',')
                logger.info(f"Found CSV header with {len(headers)} columns: {headers}")
                
                # Map the columns we need
                header_info = {
                    'codes_index': headers.index('codes') if 'codes' in headers else None,
                    'verified_index': headers.index('verified') if 'verified' in headers else None,
                    'type_index': headers.index('type') if 'type' in headers else None,
                    'id_index': headers.index('id') if 'id' in headers else None
                }
                
                # Validate that we have all required columns
                missing_columns = [col for col, idx in header_info.items() if idx is None]
                if missing_columns:
                    logger.error(f"Missing required columns in CSV header: {missing_columns}")
                    return None
                
                logger.info(f"Successfully parsed header: {header_info}")
                return header_info
            else:
                logger.debug(f"Line doesn't match expected header format: {line[:100]}...")
        
        logger.error("Failed to find expected CSV header format")
        return None
    
    def _process_csv_line(self, line: str, header_info: Dict, cve_ids: Set[str], results: Dict):
        """Process a single CSV line and update results."""
        try:
            # Split by comma, but handle quoted fields
            fields = self._parse_csv_line(line)
            
            codes_index = header_info['codes_index']
            verified_index = header_info['verified_index']
            type_index = header_info['type_index']
            id_index = header_info['id_index']
            
            if len(fields) <= max(codes_index, verified_index or 0, type_index or 0, id_index or 0):
                logger.debug(f"Skipping line with insufficient fields: {len(fields)} fields, need at least {max(codes_index, verified_index or 0, type_index or 0, id_index or 0) + 1}")
                return
            
            codes_field = fields[codes_index] if codes_index < len(fields) else ""
            verified_field = fields[verified_index] if verified_index and verified_index < len(fields) else "0"
            type_field = fields[type_index] if type_index and type_index < len(fields) else ""
            exploit_id = fields[id_index] if id_index and id_index < len(fields) else ""
            
            logger.debug(f"Processing line - codes: '{codes_field}', verified: '{verified_field}', type: '{type_field}', id: '{exploit_id}'")
            
            # Split codes field by common separators and normalize
            if codes_field:
                # Split by common separators: semicolon, comma, space, pipe
                code_separators = [';', ',', ' ', '|', '\t']
                codes_list = [codes_field]
                
                for separator in code_separators:
                    new_codes_list = []
                    for code in codes_list:
                        new_codes_list.extend(code.split(separator))
                    codes_list = new_codes_list
                
                # Clean and normalize codes
                normalized_codes = []
                for code in codes_list:
                    code = code.strip().upper()
                    if code:  # Skip empty codes
                        normalized_codes.append(code)
                
                logger.debug(f"Parsed codes field '{codes_field}' into: {normalized_codes}")
            else:
                normalized_codes = []
                logger.debug("Empty codes field")
            
            # Check if any of our CVEs are in the normalized codes list
            for cve_id in cve_ids:
                cve_upper = cve_id.upper()
                
                # Check both exact match and partial match in normalized codes
                found_in_codes = False
                if cve_upper in normalized_codes:
                    found_in_codes = True
                    logger.debug(f"Exact match found for {cve_id} in normalized codes")
                else:
                    # Also check if CVE is contained within any of the codes (for partial matches)
                    for code in normalized_codes:
                        if cve_upper in code or code in cve_upper:
                            found_in_codes = True
                            logger.debug(f"Partial match found for {cve_id} in code '{code}'")
                            break
                
                if found_in_codes:
                    # Check if this is a verified exploit
                    is_verified = verified_field.strip() == "1"
                    
                    # Check if this is a working exploit type
                    is_working_exploit = type_field.lower() in ['exploit', 'dos', 'shellcode']
                    
                    if cve_id not in results:
                        results[cve_id] = {
                            'exploit_available': True,
                            'exploit_public': True,
                            'exploit_verified': is_verified,
                            'links': [],
                            'exploit_count': 0,
                            'verified_count': 0,
                            'working_count': 0
                        }
                    
                    # Update counts
                    results[cve_id]['exploit_count'] += 1
                    if is_verified:
                        results[cve_id]['verified_count'] += 1
                    if is_working_exploit:
                        results[cve_id]['working_count'] += 1
                    
                    # Add exploit URL if available
                    if exploit_id:
                        exploit_url = f"https://www.exploit-db.com/exploits/{exploit_id}"
                        if exploit_url not in results[cve_id]['links']:
                            results[cve_id]['links'].append(exploit_url)
                    
                    logger.info(f"Found exploit for {cve_id} in Exploit-DB CSV (codes: {codes_field}, verified: {is_verified}, type: {type_field})")
                else:
                    logger.debug(f"No match found for {cve_id} in codes: {normalized_codes}")
        
        except Exception as e:
            logger.error(f"Error parsing line in Exploit-DB CSV: {str(e)}")
            logger.error(f"Problematic line: {line[:200]}...")
            return
    
    def _check_exploit_db(self, cve_id: str) -> Optional[Dict]:
        """Check a single CVE against Exploit-DB using CSV file."""
        try:
            # Use the bulk method for single CVE
            results = self._check_exploit_db_bulk({cve_id})
            return results.get(cve_id)
        except Exception as e:
            logger.debug(f"Failed to check Exploit-DB for {cve_id}: {str(e)}")
            return None
    
    def _parse_csv_line(self, line: str) -> List[str]:
        """Parse a CSV line, handling quoted fields properly."""
        fields = []
        current_field = ""
        in_quotes = False
        
        for char in line:
            if char == '"':
                in_quotes = not in_quotes
            elif char == ',' and not in_quotes:
                fields.append(current_field)
                current_field = ""
            else:
                current_field += char
        
        # Add the last field
        fields.append(current_field)
        
        return fields
        
        # Use ThreadPoolExecutor for parallel requests
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_cve = {executor.submit(check_single_cve, cve_id): cve_id for cve_id in cve_ids}
            
            for future in as_completed(future_to_cve):
                cve_id, result = future.result()
                if result:
                    results[cve_id] = result
        
        return results
    
    def _check_nvd_bulk(self, cve_ids: Set[str]) -> Dict[str, Dict]:
        """Check multiple CVEs against NVD efficiently."""
        results = {}
        
        def check_single_cve(cve_id: str) -> Tuple[str, Optional[Dict]]:
            try:
                self._rate_limit("nvd.nist.gov", 0.5)
                url = f"https://services.nvd.nist.gov/rest/json/cves/2.0?cveId={cve_id}"
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                data = response.json()
                
                if data.get('totalResults', 0) > 0:
                    # NVD provides vulnerability information, not exploit information
                    # So we'll just return that the CVE exists in NVD
                    return cve_id, {
                        'links': [f"https://nvd.nist.gov/vuln/detail/{cve_id}"]
                    }
                else:
                    return cve_id, None
                    
            except Exception as e:
                logger.debug(f"Failed to check NVD for {cve_id}: {str(e)}")
                return cve_id, None
        
        # Use ThreadPoolExecutor for parallel requests
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_cve = {executor.submit(check_single_cve, cve_id): cve_id for cve_id in cve_ids}
            
            for future in as_completed(future_to_cve):
                cve_id, result = future.result()
                if result:
                    results[cve_id] = result
        
        return results
    
    def get_epss_data(self, cve_ids: List[str]) -> Dict[str, Dict]:
        """
        Get EPSS (Exploit Prediction Scoring System) data from FIRST API.
        
        Args:
            cve_ids: List of CVE identifiers
            
        Returns:
            Dictionary mapping CVE IDs to EPSS data
        """
        results = {}
        
        if not cve_ids:
            return results
        
        logger.info(f"Fetching EPSS data for {len(cve_ids)} CVEs from FIRST API")
        
        # Check cache first
        cached_results = self._get_cached_epss(cve_ids)
        results.update(cached_results)
        
        # Find CVEs that are not in cache
        missing_cves = [cve_id for cve_id in cve_ids if cve_id not in cached_results]
        
        if not missing_cves:
            logger.info(f"All {len(cve_ids)} CVEs found in EPSS cache")
            return results
        
        logger.info(f"Need to fetch EPSS data for {len(missing_cves)} CVEs from API")
        
        try:
            # FIRST API supports up to 2000 characters in cve parameter
            # Split CVEs into chunks if needed
            max_cves_per_request = 100  # Conservative limit
            cve_chunks = [missing_cves[i:i + max_cves_per_request] for i in range(0, len(missing_cves), max_cves_per_request)]
            
            for chunk in cve_chunks:
                # Build query parameters
                cve_param = ','.join(chunk)
                
                # FIRST API endpoint
                url = "https://api.first.org/data/v1/epss"
                params = {
                    'cve': cve_param,
                    'scope': 'public',  # Get basic EPSS data
                    'pretty': 'false'   # Compact response
                }
                
                logger.debug(f"Requesting EPSS data for chunk: {chunk[:5]}{'...' if len(chunk) > 5 else ''}")
                
                # Rate limiting for FIRST API
                self._rate_limit("api.first.org", 0.5)  # 2 requests per second
                
                response = self.session.get(url, params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                if data.get('status') == 'OK' and 'data' in data:
                    chunk_results = {}
                    logger.info(f"Processing {len(data['data'])} EPSS records from FIRST API")
                    
                    for item in data['data']:
                        cve_id = item.get('cve')
                        if cve_id:
                            epss_score = float(item.get('epss', 0))
                            epss_percentile = float(item.get('percentile', 0))
                            
                            chunk_results[cve_id] = {
                                'epss_score': epss_score,
                                'epss_percentile': epss_percentile,
                                'epss_date': item.get('date'),
                                'epss_created': item.get('created'),
                                'data_source': 'FIRST-EPSS'
                            }
                            logger.debug(f"Processed EPSS data for {cve_id}: score={epss_score}, percentile={epss_percentile}")
                    
                    # Update results and cache
                    results.update(chunk_results)
                    self._save_epss_cache(chunk_results)
                    
                    logger.info(f"Retrieved EPSS data for {len(chunk_results)} CVEs from chunk")
                    logger.debug(f"Chunk results: {chunk_results}")
                else:
                    logger.warning(f"FIRST API returned unexpected response: {data.get('status', 'Unknown')}")
                    logger.debug(f"Full response: {data}")
                
                # Small delay between chunks to be respectful
                time.sleep(0.1)
            
            logger.info(f"Successfully retrieved EPSS data for {len(results)} CVEs (cached: {len(cached_results)}, new: {len(results) - len(cached_results)})")
            return results
            
        except requests.RequestException as e:
            logger.error(f"Failed to fetch EPSS data from FIRST API: {str(e)}")
            return results
        except Exception as e:
            logger.error(f"Error processing EPSS data from FIRST API: {str(e)}")
            return results
    
    def get_epss_data_bulk(self, cve_ids: List[str]) -> Dict[str, Dict]:
        """
        Get EPSS data for multiple CVEs efficiently.
        This method is optimized for bulk operations.
        
        Args:
            cve_ids: List of CVE identifiers
            
        Returns:
            Dictionary mapping CVE IDs to EPSS data
        """
        return self.get_epss_data(cve_ids)
    
    def collect_vulnerability_data_bulk(self, cve_ids: List[str]) -> Dict[str, Tuple[Optional[Dict], Optional[Dict]]]:
        """
        Collect vulnerability data for multiple CVEs efficiently.
        
        Args:
            cve_ids: List of CVE identifiers
            
        Returns:
            Dictionary mapping CVE IDs to (cve_details, exploit_info) tuples
        """
        cve_set = set(cve_ids)
        results = {cve_id: (None, None) for cve_id in cve_ids}
        
        logger.info(f"Starting bulk collection for {len(cve_ids)} CVEs")
        
        # Collect EPSS data from FIRST API (most efficient for bulk)
        logger.info("Collecting EPSS data from FIRST API")
        epss_results = self.get_epss_data_bulk(cve_ids)
        logger.info(f"Retrieved EPSS data for {len(epss_results)} CVEs")
        
        # Collect CISA KEV data in bulk (most efficient)
        kev_results = self._check_cisa_kev_bulk(cve_set)
        logger.info(f"Found {len(kev_results)} CVEs in CISA KEV")
        
        # Collect exploit data in parallel
        exploit_db_results = self._check_exploit_db_bulk(cve_set)
        nvd_results = self._check_nvd_bulk(cve_set)
        
        # Combine results
        for cve_id in cve_ids:
            exploit_info = {
                'exploit_available': False,
                'exploit_public': False,
                'exploit_verified': False,
                'exploit_links': []
            }
            
            # Add EPSS data if available
            if cve_id in epss_results:
                epss_data = epss_results[cve_id]
                # EPSS data will be integrated into CVE details later
                logger.debug(f"EPSS data for {cve_id}: score={epss_data.get('epss_score')}, percentile={epss_data.get('epss_percentile')}")
            
            # Add CISA KEV data
            if cve_id in kev_results:
                kev_data = kev_results[cve_id]
                exploit_info.update(kev_data)
                exploit_info['exploit_available'] = True
                exploit_info['exploit_verified'] = True
            
            # Add Exploit-DB data
            if cve_id in exploit_db_results:
                exploit_db_data = exploit_db_results[cve_id]
                exploit_info.update(exploit_db_data)
            
            # Add NVD data
            if cve_id in nvd_results:
                nvd_data = nvd_results[cve_id]
                exploit_info['exploit_links'].extend(nvd_data.get('links', []))
            
            # For now, skip CVE details in bulk mode (too many individual requests)
            # In production, you might want to implement a bulk CVE API or cache
            cve_details = None
            
            results[cve_id] = (cve_details, exploit_info)
        
        logger.info(f"Completed bulk collection for {len(cve_ids)} CVEs")
        return results


def collect_vulnerability_data(cve_id: str) -> Tuple[Optional[Dict], Optional[Dict]]:
    """
    Collect comprehensive vulnerability data from multiple sources.
    
    Args:
        cve_id: The CVE identifier
        
    Returns:
        Tuple of (cve_details, exploit_info) or (None, None) if failed
    """
    collector = VulnerabilityDataCollector()
    
    # Add delay to be respectful to APIs
    time.sleep(1)
    
    # Check if we already have recent EPSS data in database
    from django.utils import timezone
    from datetime import timedelta
    
    # Try to get existing vulnerability details to check EPSS data freshness
    try:
        from ..models import Vulnerability, VulnerabilityDetails
        vulnerability = Vulnerability.objects.filter(vulnerability_id=cve_id).first()
        if vulnerability:
            try:
                existing_details = vulnerability.details
                # Check if EPSS data is recent (less than 7 days old)
                if (existing_details.epss_score is not None and 
                    existing_details.epss_last_updated and 
                    timezone.now() - existing_details.epss_last_updated < timedelta(days=7)):
                    
                    logger.info(f"Using existing EPSS data for {cve_id} (age: {(timezone.now() - existing_details.epss_last_updated).days} days)")
                    # Return existing EPSS data without API call
                    existing_epss_data = {
                        cve_id: {
                            'epss_score': existing_details.epss_score,
                            'epss_percentile': existing_details.epss_percentile,
                            'epss_date': existing_details.epss_date,
                            'epss_created': existing_details.epss_created,
                            'data_source': existing_details.epss_data_source
                        }
                    }
                else:
                    existing_epss_data = {}
            except VulnerabilityDetails.DoesNotExist:
                existing_epss_data = {}
        else:
            existing_epss_data = {}
    except Exception as e:
        logger.warning(f"Could not check existing EPSS data for {cve_id}: {e}")
        existing_epss_data = {}
    
    # Only collect EPSS data if we don't have recent data
    if not existing_epss_data:
        logger.info(f"Collecting fresh EPSS data for {cve_id}")
        epss_data = collector.get_epss_data([cve_id])
    else:
        logger.info(f"Using existing EPSS data for {cve_id}")
        epss_data = existing_epss_data
    
    # Collect CVE details and exploit info
    cve_details = collector.get_cve_details(cve_id)
    exploit_info = collector.get_exploit_info(cve_id)
    
    # Integrate EPSS data into CVE details if available
    if cve_id in epss_data:
        epss_info = epss_data[cve_id]
        logger.info(f"Integrating EPSS data for {cve_id}: {epss_info}")
        
        if cve_details:
            cve_details.update({
                'epss_score': epss_info.get('epss_score'),
                'epss_percentile': epss_info.get('epss_percentile'),
                'epss_date': epss_info.get('epss_date'),
                'epss_created': epss_info.get('epss_created'),
                'epss_data_source': epss_info.get('data_source')
            })
            logger.info(f"Updated existing CVE details with EPSS: {cve_details}")
        else:
            # Create CVE details if they don't exist
            cve_details = {
                'epss_score': epss_info.get('epss_score'),
                'epss_percentile': epss_info.get('epss_percentile'),
                'epss_date': epss_info.get('epss_date'),
                'epss_created': epss_info.get('epss_created'),
                'epss_data_source': epss_info.get('data_source')
            }
            logger.info(f"Created new CVE details with EPSS: {cve_details}")
        
        logger.info(f"Final CVE details after EPSS integration: {cve_details}")
    else:
        logger.info(f"No EPSS data found for {cve_id}")
    
    return cve_details, exploit_info


def collect_vulnerability_data_bulk(cve_ids: List[str]) -> Dict[str, Tuple[Optional[Dict], Optional[Dict]]]:
    """
    Collect vulnerability data for multiple CVEs efficiently.
    
    Args:
        cve_ids: List of CVE identifiers
        
    Returns:
        Dictionary mapping CVE IDs to (cve_details, exploit_info) tuples
    """
    collector = VulnerabilityDataCollector()
    return collector.collect_vulnerability_data_bulk(cve_ids) 


def test_exploit_db_search():
    """
    Test function to verify Exploit-DB search algorithm.
    This function can be used for debugging and testing.
    """
    collector = VulnerabilityDataCollector()
    
    # Test CVE that should be found
    test_cve = "CVE-2000-0873"
    logger.info(f"Testing Exploit-DB search for {test_cve}")
    
    try:
        result = collector._check_exploit_db(test_cve)
        if result:
            logger.info(f" SUCCESS: Found {test_cve} in Exploit-DB")
            logger.info(f"Result: {result}")
        else:
            logger.error(f" FAILED: {test_cve} not found in Exploit-DB")
            
        return result
        
    except Exception as e:
        logger.error(f" ERROR: Failed to test {test_cve}: {str(e)}")
        return None


def test_epss_integration():
    """
    Test function to verify EPSS data integration.
    This function can be used for debugging and testing.
    """
    collector = VulnerabilityDataCollector()
    
    # Test CVE that should have EPSS data
    test_cve = "CVE-2021-44228"  # Log4Shell - should have EPSS data
    logger.info(f"Testing EPSS integration for {test_cve}")
    
    try:
        # Test EPSS data collection
        epss_data = collector.get_epss_data([test_cve])
        logger.info(f"EPSS data collected: {epss_data}")
        
        # Test full vulnerability data collection
        cve_details, exploit_info = collect_vulnerability_data(test_cve)
        logger.info(f"CVE details: {cve_details}")
        logger.info(f"Exploit info: {exploit_info}")
        
        if cve_details and test_cve in epss_data:
            logger.info(" SUCCESS: EPSS data integrated into CVE details")
            logger.info(f"EPSS score: {cve_details.get('epss_score')}")
            logger.info(f"EPSS source: {cve_details.get('epss_data_source')}")
        else:
            logger.warning(" WARNING: EPSS data not properly integrated")
            
        return cve_details, exploit_info
        
    except Exception as e:
        logger.error(f" ERROR: Failed to test EPSS integration for {test_cve}: {str(e)}")
        return None, None


# Add test function to module namespace
__all__ = [
    'VulnerabilityDataCollector',
    'collect_vulnerability_data',
    'collect_vulnerability_data_bulk',
    'test_exploit_db_search',
    'test_epss_integration'
] 